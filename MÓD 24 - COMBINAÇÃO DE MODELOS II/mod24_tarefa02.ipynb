{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e01fe748-314a-4fa9-bbc8-b58cd625898d",
      "metadata": {
        "id": "e01fe748-314a-4fa9-bbc8-b58cd625898d"
      },
      "source": [
        "[![ebac_logo-data_science.png](https://raw.githubusercontent.com/rhatiro/Curso_EBAC-Profissao_Cientista_de_Dados/main/ebac-course-utils/media/logo/ebac_logo-data_science.png)](https://github.com/rhatiro/Curso_EBAC-Profissao_Cientista_de_Dados)\n",
        "<!-- <img src=\"https://raw.githubusercontent.com/rhatiro/Curso_EBAC-Profissao_Cientista_de_Dados/main/ebac-course-utils/media/logo/ebac_logo-data_science.png\" alt=\"ebac_logo-data_science\"> -->\n",
        "\n",
        "---\n",
        "\n",
        "<!-- # **Profissão: Cientista de Dados** -->\n",
        "### **Módulo 24** | Combinação de modelos II | Exercício 2\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da87421f-2533-4f0f-9df1-61d76430e0bc",
      "metadata": {
        "id": "da87421f-2533-4f0f-9df1-61d76430e0bc"
      },
      "source": [
        "# Tarefa 02"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76f8c515-70bd-4e3d-a35e-267fc6101275",
      "metadata": {
        "id": "76f8c515-70bd-4e3d-a35e-267fc6101275"
      },
      "source": [
        "**1.** Cite 5 diferenças entre o AdaBoost e o GBM."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Adaboost**: Utiliza uma floresta de árvores fracas (Stumps)\n",
        "\n",
        "   **GBM**: Utiliza uma floresta de árvores mais complexas.\n",
        "\n",
        "\n",
        "2. **AdaBoost:** O primeiro passo no método é um Stump\n",
        "\n",
        "   **GBM**: O primeiro passo no método é a média do Y\n",
        "\n",
        "\n",
        "3. **AdaBoost**: Cada resposta de cada modelo (Stump) tem peso diferente\n",
        "\n",
        "   **GBM**: Toda resposta das árvores tem um multiplicador em comum chamado '*learning_rate*' (eta)\n",
        "\n",
        "\n",
        "4. **AdaBoost**: Menos robusto a outliers, pois os erros são penalizados exponencialmente, o que pode levar o modelo a focar excessivamente em outliers.\n",
        "  \n",
        "  **GBM**: Pode ser mais robusto a outliers, especialmente com a escolha adequada da função de perda e hiperparâmetros.\n",
        "\n",
        "\n",
        "5. **AdaBoost**: É relativamente simples e menos flexível em termos de ajuste fino. A principal variação é a escolha do modelo base e o número de estimadores.\n",
        "\n",
        "   **GBM**: Muito mais flexível e extensível permitindo ajustar muitos hiperparâmetros.\n"
      ],
      "metadata": {
        "id": "xpYKbVgxe_73"
      },
      "id": "xpYKbVgxe_73"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.** Acesse o link Scikit-learn–GBM, leia a explicação (traduza se for preciso) e crie um jupyternotebook contendo o exemplo de classificação e de regressão do GBM."
      ],
      "metadata": {
        "id": "FsxqRXjSevdT"
      },
      "id": "FsxqRXjSevdT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classificação"
      ],
      "metadata": {
        "id": "1GzPDUoORDtw"
      },
      "id": "1GzPDUoORDtw"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6c18906a-757b-4c3b-8c96-d810bcafbd81",
      "metadata": {
        "id": "6c18906a-757b-4c3b-8c96-d810bcafbd81"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.datasets import make_hastie_10_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d41a352d-e340-44bd-b6bf-431f9ac42aba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d41a352d-e340-44bd-b6bf-431f9ac42aba",
        "outputId": "567daf25-ecd0-4d0c-d2f7-1011b589a904"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8965"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "X, y = make_hastie_10_2(random_state=0)\n",
        "X_train, X_test = X[:2000], X[2000:]\n",
        "y_train, y_test = y[:2000], y[2000:]\n",
        "\n",
        "clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n",
        "clf.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regressão"
      ],
      "metadata": {
        "id": "vXHhj4OcRFSL"
      },
      "id": "vXHhj4OcRFSL"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "39199baf-62e1-493b-a692-0fb21529054d",
      "metadata": {
        "id": "39199baf-62e1-493b-a692-0fb21529054d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_friedman1\n",
        "from sklearn.ensemble import GradientBoostingRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a068a940-b305-4c6a-bccb-cef99e59d0af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a068a940-b305-4c6a-bccb-cef99e59d0af",
        "outputId": "019f80b4-e26b-444c-cf3e-cbbfb269df52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.009154859960321"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
        "\n",
        "X_train, X_test = X[:200], X[200:]\n",
        "y_train, y_test = y[:200], y[200:]\n",
        "\n",
        "est = GradientBoostingRegressor(\n",
        "      n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\n",
        "      loss='squared_error').fit(X_train, y_train)\n",
        "\n",
        "mean_squared_error(y_test, est.predict(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "875633aa-d96e-460b-8487-a6743ef61926",
      "metadata": {
        "id": "875633aa-d96e-460b-8487-a6743ef61926"
      },
      "source": [
        "**3.** Cite 5 hiperparâmetros importantes no GBM."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c86622a0-b73c-4c06-84dd-76f20ddf2fe9",
      "metadata": {
        "id": "c86622a0-b73c-4c06-84dd-76f20ddf2fe9"
      },
      "source": [
        "> **1. `learning_rate`** Este hiperparâmetro escala a contribuição de cada árvore. Uma taxa de aprendizado menor geralmente requer um número maior de árvores para modelar bem os dados, o que pode melhorar a generalização.\n",
        ">\n",
        "> **2. `n_estimators`**: Este hiperparâmetro especifica o número de estágios de boosting a serem executados. Aumentar o número de estimadores geralmente melhora o desempenho, mas também aumenta o risco de overfitting e o custo computacional.\n",
        ">\n",
        "> **3. `subsample`**: Este hiperparâmetro representa a fração de amostras usadas para ajustar cada estimador base individual. Definir este valor para menos de 1,0 leva ao Gradient Boosting Estocástico, o que pode ajudar a evitar overfitting ao adicionar aleatoriedade ao modelo.\n",
        ">\n",
        "> **4. `min_samples_split`**: Especifica o número mínimo de amostras necessário para dividir um nó interno. Isso pode afetar a granularidade das divisões e, portanto, a complexidade do modelo.\n",
        ">\n",
        "> **5. `max_depth`**:  Controla a profundidade máxima de cada árvore de decisão individual. Limitar a profundidade das árvores ajuda a evitar o overfitting. Árvores mais rasas geralmente correspondem a um modelo mais regularizado.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "686d6267-19e7-47d2-b795-a68bcd361515",
      "metadata": {
        "id": "686d6267-19e7-47d2-b795-a68bcd361515"
      },
      "source": [
        "**4.** (**Opcional**) Utilize o GridSearch para encontrar os melhores hiperparâmetros para o conjunto de dados do exemplo."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.datasets import make_hastie_10_2\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "X, y = make_hastie_10_2(random_state=0)\n",
        "\n",
        "# Divida o conjunto de dados em conjuntos de treinamento e teste\n",
        "X_train, X_test = X[:2000], X[2000:]\n",
        "y_train, y_test = y[:2000], y[2000:]\n",
        "\n",
        "# Defina a grade de parâmetros\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_leaf': [20, 50, 100],\n",
        "}\n",
        "\n",
        "hgb = HistGradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Avaliando o melhor modelo no conjunto de teste\n",
        "\n",
        "grid_search = GridSearchCV(estimator=hgb,\n",
        "                           param_grid=param_grid,\n",
        "                           scoring='roc_auc',\n",
        "                           n_jobs=-1,\n",
        "                           cv=5,\n",
        "                           verbose=3)\n",
        "\n",
        "# Ajuste o modelo usando GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Produza os melhores parâmetros e melhor pontuação\n",
        "print(\"Melhores parâmetros: \", grid_search.best_params_)\n",
        "print(\"Melhor Score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2daPPtRDjG7m",
        "outputId": "078b5ad5-d166-4c62-e278-a1d56c1cc36d"
      },
      "id": "2daPPtRDjG7m",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhores parâmetros:  {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_leaf': 50}\n",
            "Melhor Score:  0.9646724075592077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bd77068-5da9-48bf-b7b7-c88ff2978b5e",
      "metadata": {
        "id": "2bd77068-5da9-48bf-b7b7-c88ff2978b5e"
      },
      "source": [
        "**5.** Acessando o artigo do Jerome Friedman ([Stochastic](https://jerryfriedman.su.domains/ftp/stobst.pdf)) e pensando no nome dado ao **Stochastic GBM**, qual é a maior diferença entre os dois algoritmos?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f3fde36-2bb7-46f9-ba26-55627bea0d8a",
      "metadata": {
        "id": "7f3fde36-2bb7-46f9-ba26-55627bea0d8a"
      },
      "source": [
        "A principal diferença entre o Gradient Boosting convencional e o Stochastic Gradient Boosting, como descrito no artigo de Jerome Friedman, é a introdução de aleatoriedade na seleção de amostras de treinamento para cada iteração do modelo.\n",
        "\n",
        "Em cada iteração, o Gradient Boosting convencional utiliza todo o conjunto de dados de treinamento para ajustar a próxima árvore, já o Stochastic Gradient Boosting utiliza apenas uma fração (subconjunto) do conjunto de dados de treinamento para ajustar o próximo modelo. Este subconjunto é selecionado aleatoriamente.\n",
        "\n",
        "Gradient Boosting Convencional usa o conjunto de dados completo em cada iteração, enquanto o Stochastic Gradient Boosting utiliza apenas uma fração aleatória dos dados, introduzindo variabilidade que pode levar a uma melhor generalização e menor risco de overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aa4448a-5921-4d47-84af-b29a7f008175",
      "metadata": {
        "id": "2aa4448a-5921-4d47-84af-b29a7f008175"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}