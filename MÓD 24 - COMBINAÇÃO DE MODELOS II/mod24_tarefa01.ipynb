{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "26d9d7e5-dd35-4222-9f5b-2ad51ce3291f",
      "metadata": {
        "id": "26d9d7e5-dd35-4222-9f5b-2ad51ce3291f"
      },
      "source": [
        "[![ebac_logo-data_science.png](https://raw.githubusercontent.com/rhatiro/Curso_EBAC-Profissao_Cientista_de_Dados/main/ebac-course-utils/media/logo/ebac_logo-data_science.png)](https://github.com/rhatiro/Curso_EBAC-Profissao_Cientista_de_Dados)\n",
        "<!-- <img src=\"https://raw.githubusercontent.com/rhatiro/Curso_EBAC-Profissao_Cientista_de_Dados/main/ebac-course-utils/media/logo/ebac_logo-data_science.png\" alt=\"ebac_logo-data_science\"> -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbc89696-b7e1-4be3-b493-c6b863fa337f",
      "metadata": {
        "id": "fbc89696-b7e1-4be3-b493-c6b863fa337f"
      },
      "source": [
        "# Tarefa 01"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05220745-5553-46bf-9721-3e16cccd3285",
      "metadata": {
        "id": "05220745-5553-46bf-9721-3e16cccd3285"
      },
      "source": [
        "**1.** Cite 5 diferenças entre o Random Forest e o AdaBoost."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1)**\n",
        "  * **Random Forest:** Utiliza um conjunto de árvores de decisão, e essas árvores são completas e com possibildiade de pódas.\n",
        "\n",
        "  * **Adaboost:** Utiliza árvores de decisão simples, com apenas 1 de profundidade e 2 folhas, que podem ser chamada de *Stumps* (classificadores fracos)\n",
        "\n",
        "**2)**\n",
        "  * Random Forest: As árvores são treinadas paralelamente e são todas independentes uma das outras.\n",
        "  \n",
        "  * Adaboost: Os *stumps* (árvores de decisão simples) influenciam uma nas outras.\n",
        "\n",
        "**3)**\n",
        "  * Random Forest: A resposta das árvores tem o mesmo peso por serem treinadas de forma mais robusta.\n",
        "\n",
        "  *Adaboost: As respostas de cada árvore tem peso diferente, com pesos ajustados para focar nas instâncias classificadas incorretamente nas iterações anteriores.\n",
        "\n",
        "**4)**\n",
        "  * Random Forest: os conjuntos de dados treinados são obtidos através do bootstrap (amostragem aleatória com reposição)\n",
        "\n",
        "  * AdaBoost: os conjuntos de dados são obtidos também pelo bootstrap, porém os dados são ponderados através do \"peso\" de cada durante o cálculo de performance de cada Stump, e as linhas classificadas incorretamente tem maior peso e maior probabilidade de serém escolhidas.\n",
        "\n",
        "**5)**\n",
        "  * Random Forest: É feita uma votação majoritária entre as árvores de decisão do conjunto para o resultado final\n",
        "\n",
        "  * AdaBoost: É feito uma votação ponderada de cada resposta de acordo com a performance de cada *Stump*\n",
        "\n"
      ],
      "metadata": {
        "id": "b2-otDu2bO26"
      },
      "id": "b2-otDu2bO26"
    },
    {
      "cell_type": "markdown",
      "id": "89239d72-96bc-47e4-b991-b1e709be253c",
      "metadata": {
        "id": "89239d72-96bc-47e4-b991-b1e709be253c"
      },
      "source": [
        "**2.** Acesse o link [Scikit-learn – adaboost](https://scikit-learn.org/stable/modules/ensemble.html), leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo do AdaBoost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "10a1819c-32c7-4923-a726-316782116e62",
      "metadata": {
        "id": "10a1819c-32c7-4923-a726-316782116e62"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets        import load_iris\n",
        "from sklearn.ensemble        import AdaBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0424eee9-4d7c-4b5d-a49e-db5a1aba574f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0424eee9-4d7c-4b5d-a49e-db5a1aba574f",
        "outputId": "dce96144-7a13-4e14-e3ef-72fe1d35c49c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9533333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "clf = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\",)\n",
        "\n",
        "scores = cross_val_score(clf,\n",
        "                         X,\n",
        "                         y,\n",
        "                         cv=5)\n",
        "\n",
        "scores.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d49e8a9-17c5-4a26-8acc-c73363ef763e",
      "metadata": {
        "id": "1d49e8a9-17c5-4a26-8acc-c73363ef763e"
      },
      "source": [
        "**3.** Cite 5 Hiperparâmetros importantes no AdaBoost."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1efa6a0-234a-4c6e-a673-320350573444",
      "metadata": {
        "id": "d1efa6a0-234a-4c6e-a673-320350573444"
      },
      "source": [
        "> 1. n_estimators:\n",
        "Este hiperparâmetro controla o número de estimadores (classificadores fracos) a serem criados durante o treinamento do AdaBoost. Um valor maior de n_estimators pode aumentar a complexidade do modelo, mas também pode levar a um melhor desempenho, até certo ponto.\n",
        "\n",
        "> 2. base_estimator:\n",
        "O base_estimator é o tipo de estimador fraco a ser utilizado em cada iteração do AdaBoost. Geralmente, é uma árvore de decisão simples (por exemplo, DecisionTreeClassifier) ou um modelo linear (por exemplo, LogisticRegression). A escolha do estimador base pode afetar significativamente o desempenho do AdaBoost.\n",
        "\n",
        "> 3. learning_rate:\n",
        "O learning_rate controla a taxa de aprendizado do AdaBoost, ou seja, a contribuição de cada estimador fraco para a atualização dos pesos das instâncias de dados. Um valor menor de learning_rate significa que os estimadores fracos contribuem menos para a predição final, o que pode ajudar a regularizar o modelo.\n",
        "\n",
        "> 4. algorithm:\n",
        "O algorithm especifica o algoritmo a ser usado para atualizar os pesos das instâncias de dados durante o treinamento do AdaBoost. Os valores comuns incluem 'SAMME' (Stagewise Additive Modeling using a Multiclass Exponential loss function) e 'SAMME.R' (versão de SAMME que utiliza probabilidades reais).\n",
        "\n",
        "> 5. random_state:\n",
        "O random_state é usado para controlar a aleatoriedade no processo de treinamento do AdaBoost. Definir um valor fixo para random_state garante reprodutibilidade dos resultados, pois garante que as mesmas divisões aleatórias sejam geradas em diferentes execuções."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f22e83d-a610-42d5-b995-a1d238f5c1d5",
      "metadata": {
        "id": "9f22e83d-a610-42d5-b995-a1d238f5c1d5"
      },
      "source": [
        "**4.** (**Opcional**) Utilize o GridSearch para encontrar os melhores hiperparâmetros para o conjunto de dados do exemplo (load_iris)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "417b7e95-c484-40cf-bc26-0fdd88e5d6a4",
      "metadata": {
        "id": "417b7e95-c484-40cf-bc26-0fdd88e5d6a4"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "clf = AdaBoostClassifier()\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': list(range(1, 1001, 50)),\n",
        "    'algorithm': [\"SAMME\", 'SAMME.R']\n",
        "}\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5)\n",
        "\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "print(\"Melhores parâmetros:\", grid_search.best_params_)\n",
        "print(\"Melhor score:\", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roZn5pJ1hBuU",
        "outputId": "ff371310-a8d7-4ead-fc53-5b415975d1f9"
      },
      "id": "roZn5pJ1hBuU",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'algorithm': 'SAMME', 'n_estimators': 51}\n",
            "Best score: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "921159d5-086f-44d0-a8dc-1f222c0e7d9e",
      "metadata": {
        "id": "921159d5-086f-44d0-a8dc-1f222c0e7d9e"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}